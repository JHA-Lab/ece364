{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1_ECE364.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"E32UBMT7VKMm"},"source":["## Prepare python environment\n"]},{"cell_type":"code","metadata":{"id":"y_lm7Q-9VKMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryOZJYQa3PG0"},"source":["random_state=5 # use this to control randomness across runs e.g., dataset partitioning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BASGXrOy4wat"},"source":["## Preparing the Statslog (Heart) Dataset (1 points)\n","\n","---\n","\n","\n","We will use heart dataset from UCI machine learning repository. Details of this data can be found [here](https://archive.ics.uci.edu/ml/datasets/statlog+(heart)). \n","The dataset contains the following features with their corresponding feature types:\n","1. age in years (real)\n","2. sex (binary; 1=male/0=female)\n","3. cp: chest pain type (categorical)\n","4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) (real)\n","5. chol: serum cholestorol in mg/dl (real)\n","6. fbs: (fasting blood sugar > 120 mg/dl) (binary; 1=true/0=false)\n","7. restecg: resting electrocardiographic results (categorical)\n","8. thalach: maximum heart rate achieved (real)\n","9. exang: exercise induced angina (1 = yes; 0 = no) (binary)\n","10. oldpeak: ST depression induced by exercise relative to rest (real)\n","11. slope: the slope of the peak exercise ST segment (ordinal)\n","12. ca: number of major vessels colored by flourosopy (real)\n","13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect. (categorical)\n","\n","The objective is to determine whether a person has heart disease or not based on these features.\n","\n","Note: We will use a subset of the above features because the [scikit-learn implementation of Decision Trees does not support categorical variables](https://scikit-learn.org/stable/modules/tree.html#tree). "]},{"cell_type":"markdown","metadata":{"id":"URgO9HCN6RCl"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"id":"6Pyo8XV46UlI"},"source":["# Download and load the dataset\n","import os\n","if not os.path.exists('heart.csv'): \n","    !wget https://raw.githubusercontent.com/JHA-Lab/ece364/main/dataset/heart.csv \n","df = pd.read_csv('heart.csv')\n","\n","# keep real valued features and the target feature\n","ind_non_categorical_features=np.array([0,3,4,7,9,11,-1])\n","non_categorical_features=df.columns[ind_non_categorical_features]\n","\n","df=df[non_categorical_features]\n","\n","# Display the first five instances in the dataset\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IgKnqRBm6Wap"},"source":["### Check the data type for each column"]},{"cell_type":"code","metadata":{"id":"sWDsbF6U6ZLT"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpE-lW1u6dxJ"},"source":["#### There are a total of 303 entries in this dataset. First 13 columns are features and the last column indicates whether the person has heart disease or not."]},{"cell_type":"markdown","metadata":{"id":"KS7PzOS76hrv"},"source":["#### Look at some statistics of the data using the `describe` function in pandas."]},{"cell_type":"code","metadata":{"id":"GlIARPVH6mIq"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcVuFlvZ6oYT"},"source":["1. Count tells us the number of Non-empty rows in a feature.\n","\n","2. Mean tells us the mean value of that feature.\n","\n","3. Std tells us the Standard Deviation Value of that feature.\n","\n","4. Min tells us the minimum value of that feature.\n","\n","5. 25%, 50%, and 75% are the percentile/quartile of each features.\n","\n","6. Max tells us the maximum value of that feature."]},{"cell_type":"markdown","metadata":{"id":"LT6ijHsr6tIc"},"source":["#### Look at distribution of some features across the population. See [here](https://seaborn.pydata.org/generated/seaborn.distplot.html) for details. These have been done for you."]},{"cell_type":"code","metadata":{"id":"l70LaOwd6v9g"},"source":["sns.histplot(df['thalach'],bins=30,color='red',stat=\"density\",kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOMhQWw960bh"},"source":["sns.histplot(df['chol'],bins=30,color='green',stat='density',kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35C5tgFu608o"},"source":["sns.histplot(df['trestbps'],bins=30,color='blue',stat='density',kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrT0m0GR644-"},"source":["#### Plot histogram of heart disease with age. This has been done for you."]},{"cell_type":"code","metadata":{"id":"1vStvr9G7ZmM"},"source":["plt.figure(figsize=(15,6))\n","sns.countplot(x='age',data = df, hue = 'target',palette='coolwarm_r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRhEcln77rIK"},"source":["#### Extract target and descriptive features (0.5 points)"]},{"cell_type":"code","metadata":{"id":"blhp_Upk8E-Y"},"source":["# Store all the features from the data in X\n","X= # TODO\n","# Store all the labels in y\n","y= # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdUFK3qG8Gnk"},"source":["# Convert data to numpy array\n","X = # TODO\n","y = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-JPYSnc8JQi"},"source":["#### Create training and test datasets (0.5 points)\n","\n","Split the data into training and test sets using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for testing. "]},{"cell_type":"code","metadata":{"id":"BzTzI3iT8R5x"},"source":["X_train,X_test,y_train,y_test = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0TlScQvXJaY"},"source":["## Training Decision Tree-based Classifiers (9 points)\n"]},{"cell_type":"markdown","metadata":{"id":"VKYdQa0tXJaY"},"source":["### Exercise 1: Learning a Decision Tree (5 points)\n","\n","#### We will use the `sklearn` library to train a Decision Tree classifier. Review ch.4 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for more details. "]},{"cell_type":"code","metadata":{"id":"Vd1q1gNMXJaZ"},"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Myg0DW8eXJaZ"},"source":["# tree visualization helper function \n","from sklearn.tree import export_graphviz\n","from six import StringIO  \n","from IPython.display import Image  \n","import pydotplus\n","\n","\"\"\"\n","clf: DecisionTreeClassifier\n","\n","Returns a bytes object representing the image of the tree \n","\"\"\"\n","def get_tree_image(clf):\n","    dot_data = StringIO()\n","    feature_names=df.drop('target',axis=1).columns\n","    class_names=[\"No heart disease\", \"Has heart disease\"]\n","    export_graphviz(clf, out_file=dot_data,  \n","                    filled=True, rounded=True,\n","                    special_characters=True,\n","                    feature_names=feature_names, \n","                    class_names=class_names)\n","    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n","    \n","\n","    return graph.create_png()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwSxdPYFXJaZ"},"source":["#### Exercise 1a: Fit and interpret a decision tree. (3 points)\n","\n","#### Fit Decision trees using the Gini index and entropy-based impurity measure. \n","\n","#### Set the random_state to the value defined above. Keep all other parameters at their default values. \n","\n","#### Report the training and test set accuracies for each classifier."]},{"cell_type":"code","metadata":{"id":"vfZC0Bb7XJaa"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nA9VYMOPXJaa"},"source":["#### Visualize the Decision Tree with the best test performance."]},{"cell_type":"code","metadata":{"id":"Zbn5e80hXJab"},"source":["best_clf=# TODO\n","tree_image=get_tree_image(best_clf)\n","Image(tree_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pxsC6EBIXJab"},"source":["#### Indicate the most informative descriptive feature (with the threshold) and briefly explain why this is the most informative (from an algorithmic viewpoint). "]},{"cell_type":"markdown","metadata":{"id":"LH2_jKRuXJab"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"ZMW3Dh4TXJac"},"source":["#### Briefly comment on the tree's depth and what factors may contribute to the shallowness/complexity of the tree. \n"]},{"cell_type":"markdown","metadata":{"id":"v5PBtmpRXJac"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"_PX5DzOiXJac"},"source":["#### Show how one can interpret the tree by specifying the rule from its left most branch. "]},{"cell_type":"markdown","metadata":{"id":"lfZG9R6MXJad"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"bV7egnaJXJad"},"source":["#### Exercise 1b: Prune a decision tree. (2 points)\n","\n","#### Next, let's try pruning the tree to see if we can improve the classifier's generalization performance.\n","\n","####  Preprune a decision tree by varying the `max_depth` among {None (no depth control), 1,3,5,7}.\n","\n","#### Set the criterion to entropy and the random_state to the value defined above. Keep all other parameters at their default values. \n","\n","#### Report the training and test set accuracies for each classifier."]},{"cell_type":"code","metadata":{"id":"5QE8AXWMXJad"},"source":["# TODO "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJs-XmEAXJad"},"source":["#### Analyze the effect of increasing tree depth on training and test performance."]},{"cell_type":"markdown","metadata":{"id":"Qmn3SwuSXJae"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"Ad1CZwTbXJae"},"source":["### Exercise 2: Learning an Ensemble of Decision Trees (4 points)\n","\n","#### We will use the `sklearn` library to implement bagging and boosting. Review ch.4 and read more on [bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). "]},{"cell_type":"code","metadata":{"id":"j7qulgS4XJae"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdZaCHS_XJae"},"source":["#### Exercise 2a: Fit a Random Forest. (2 points)\n","\n","#### Fit different Random Forest classifiers by varying the number of trees among {10, 100, 500,1000}. \n","\n","#### Set the `criterion` to entropy and set the random_state to the value defined above. Keep all other parameters at their default values. \n","\n","#### Report the test set accuracies for each classifier."]},{"cell_type":"code","metadata":{"id":"hGza2GOBXJaf"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6r9gRuqhXJaf"},"source":["#### Comment on the effect of increasing the number of trees on test performance. Compare the performance of the best performing Random Forest classifier against the Decision Tree Classifier trained with entropy (Ex. 1a) and explain any difference. "]},{"cell_type":"markdown","metadata":{"id":"XwmNqr1oXJaf"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"TF02vZ_5XJaf"},"source":["#### Exercise 2b: Fit a Gradient Boosted Decision Tree (GBDT). (2 points)\n","\n","#### Fit different GBDTs by varying the number of boosting steps/trees added among {5,50,100,200}. \n","\n","#### Set the `n_iter_no_change` to 100, `validation_fraction=0.2`, and random_state to the value defined above. Keep all other parameters at their default values. \n","\n","#### Report the training and test set accuracies for each classifier."]},{"cell_type":"code","metadata":{"id":"MLbv2bDtXJag"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MsG4U_vuXJag"},"source":["#### Comment on the effect of increasing the number of trees on test performance. Compare the performance of the best performing GBDT against that of the best performing Random Forest classifier (Ex. 2a) and Decision Tree classifier trained with entropy (Ex. 1a). "]},{"cell_type":"markdown","metadata":{"id":"U_SQBmp9XJah"},"source":["TO DO"]},{"cell_type":"code","metadata":{"id":"BAABiP21XJah"},"source":[""],"execution_count":null,"outputs":[]}]}