{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Assignment3_ECE364.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"E32UBMT7VKMm"},"source":["## Prepare python environment\n"]},{"cell_type":"code","metadata":{"id":"u1AE75bhOCao"},"source":["# Installs required packages\n","!apt install libgraphviz-dev\n","!pip install pomegranate matplotlib pygraphviz\n","\n","# Press \"Restart Runtime\" after running this cell, before going to the rest of the code."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_lm7Q-9VKMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryOZJYQa3PG0"},"source":["random_state=5 # use this to control randomness across runs e.g., dataset partitioning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhQMANKC53IY"},"source":["## Preparing the dataset (2 points)"]},{"cell_type":"markdown","metadata":{"id":"BASGXrOy4wat"},"source":["We will use diabetes dataset from UCI machine learning repository. Detail of this data can be found [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database). The objective of the dataset is to  predict whether or not a female patient has diabetes based on certain diagnostic measurements included in the dataset.\n","\n","The dataset consists of several medical predictor (features) variables and one target variable indicating if the person has diabetes. Predictor variables include the number of pregnancies the patient has had,  glucose level, blood pressure, skin, insulin, bmi, pedigree and age."]},{"cell_type":"markdown","metadata":{"id":"URgO9HCN6RCl"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"id":"6Pyo8XV46UlI","executionInfo":{"status":"ok","timestamp":1633898269906,"user_tz":240,"elapsed":827,"user":{"displayName":"Shikhar Tuli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08476644503706754908"}},"outputId":"fd47d28d-98a9-4090-f3a0-39019c5bce13"},"source":["# These are the names of column in the dataset. It includes all features of the data and the label.\n","col_names = ['pregnancies', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n","\n","# Download and load the dataset\n","import os\n","if not os.path.exists('diabetes.csv'): \n","    !wget https://raw.githubusercontent.com/JHA-Lab/ece364/main/dataset/diabetes.csv \n","diabetes_data = pd.read_csv(\"diabetes.csv\", header=1, names=col_names)\n","\n","FEATURE_NAMES=diabetes_data.drop('label',axis=1).columns\n","# Display the first five instances in the dataset\n","diabetes_data.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-10 20:37:48--  https://raw.githubusercontent.com/JHA-Lab/ece364/main/dataset/diabetes.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24641 (24K) [text/plain]\n","Saving to: ‘diabetes.csv’\n","\n","diabetes.csv        100%[===================>]  24.06K  --.-KB/s    in 0s      \n","\n","2021-10-10 20:37:48 (58.2 MB/s) - ‘diabetes.csv’ saved [24641/24641]\n","\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pregnancies</th>\n","      <th>glucose</th>\n","      <th>bp</th>\n","      <th>skin</th>\n","      <th>insulin</th>\n","      <th>bmi</th>\n","      <th>pedigree</th>\n","      <th>age</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>116</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25.6</td>\n","      <td>0.201</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pregnancies  glucose  bp  skin  insulin   bmi  pedigree  age  label\n","0            1       85  66    29        0  26.6     0.351   31      0\n","1            8      183  64     0        0  23.3     0.672   32      1\n","2            1       89  66    23       94  28.1     0.167   21      0\n","3            0      137  40    35      168  43.1     2.288   33      1\n","4            5      116  74     0        0  25.6     0.201   30      0"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"VqAbo9DP2dOO"},"source":["#### Use `describe` function to display some statistics of the data. See [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) details about this function."]},{"cell_type":"code","metadata":{"id":"FWOGkp8P2abU"},"source":["# Display some statistics of the data\n","diabetes_data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QTWUsK1_6t9S"},"source":["diabetes_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRhEcln77rIK"},"source":["### Extract target and descriptive features (1 point)\n"]},{"cell_type":"code","metadata":{"id":"blhp_Upk8E-Y"},"source":["#split dataset into features and target variable\n","X = # TODO \n","y = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5XDDMPY6t9U"},"source":["# Convert data to numpy array\n","X = # TODO\n","y = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-JPYSnc8JQi"},"source":["### Create training and test datasets (1 point)\n","\n","Split the data into training and test sets using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for testing. "]},{"cell_type":"code","metadata":{"id":"BzTzI3iT8R5x"},"source":["X_train,X_test,y_train,y_test = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQTCntOk6t9W"},"source":["## Training probability-based classifiers (18 points)\n"]},{"cell_type":"markdown","metadata":{"id":"sDcVqRS56t9X"},"source":["### Exercise 1: Learning a Naive Bayes Model (9 points)\n","\n","#### We will use the `pomegranate` library to train a Naive Bayes Model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/latest/NaiveBayes.html) for more details. "]},{"cell_type":"code","metadata":{"id":"CjrqGWte6t9X"},"source":["from pomegranate.distributions import NormalDistribution, ExponentialDistribution, DiscreteDistribution \n","from pomegranate.NaiveBayes import NaiveBayes\n","from pomegranate.BayesClassifier import BayesClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import KBinsDiscretizer\n","import math\n","\n","np.random.seed(random_state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wkS5cL56t9Y"},"source":["#### Exercise 1a: Fit naive bayes model using a single distribution type (2 points)\n","\n","#### Train one naive bayes model using a normal distribution per feature. Train another naive bayes model using an exponential distribution per feature. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n","\n","#### Report the training and test set accuracies for each model. Hint: use accuracy_score()\n"]},{"cell_type":"code","metadata":{"id":"LeAiEwIs6t9Y"},"source":["# TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jnm7xIL86t9Z"},"source":["#### Exercise 1b: Fit a naive bayes model using different feature distributions (3 points)\n","\n","#### Visualize the feature distributions (done for you below) to determine which distribution (normal or exponential) better models a specific feature. \n","\n","#### Train a Naive Bayes classifier using this set of feature-specific distributions. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n","\n","#### Report the training and test set accuracies for the model. Hint: use accuracy_score()"]},{"cell_type":"code","metadata":{"id":"mREYMrmg6t9Z"},"source":["# visualization code\n","\n","num_cols=4\n","num_rows=int(len(FEATURE_NAMES)/num_cols) if len(FEATURE_NAMES)%num_cols == 0 else int(math.ceil(len(FEATURE_NAMES)/num_cols))\n","fig,ax=plt.subplots(num_rows,num_cols)\n","\n","for ft_index in np.arange(X_train.shape[1]):\n","    ax[ft_index//num_cols,ft_index%num_cols].hist(X_train[:,ft_index], color='blue')\n","    ax[ft_index//num_cols,ft_index%num_cols].set_title(FEATURE_NAMES[ft_index])\n","    \n","fig.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lZTGhq16t9Z"},"source":["# TODO: train a classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lJqOu9je6t9a"},"source":["#### Comment on any performance difference between this model and the models trained in Ex. 1a. (1 point)"]},{"cell_type":"markdown","metadata":{"id":"dsYqGS4k6t9a"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"TVA5ZzhI6t9b"},"source":["#### Exercise 1c: Fit a naive bayes model on categorical features (2 points)\n","\n","#### Besides fitting a naive bayes model on the continuous features, one can fit a naive bayes model on categorical features derived from binning the continuous features, and then compute a probability mass function for each categorical feature.\n","\n","#### Bin the features by varying the strategy among {equal-width binning, equal-frequency binning}. For each binning strategy, vary the number of bins among {3,10,50}. Hint: use [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer.get_params) by modifying n_bins and strategy and setting encode=\"ordinal\" to map the labels to numerical categories.\n","\n","#### For each binning setting tried above, fit a naive bayes model on the binned version of the training set. Hint: use DiscreteDistribution to model the categorical features and NaiveBayes.from_samples() to fit the model to the data.\n","\n","#### Report the training and test set accuracy for each model trained and evaluated on binned versions of the training and test sets respectively. \n","\n","**Note** There may be some variability in the actual performance scores, but the overall trends should remain the consistent."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"_oEKSVJ86t9b"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yu_rVaWw6t9b"},"source":["#### Briefly explain any performance difference between equal-width and equal-frequency binning. Also comment on the effect of increasing the number of bins (see ch.3). (1 point)"]},{"cell_type":"markdown","metadata":{"id":"_eP9xnHa6t9c"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"cP6z3W4K6t9c"},"source":["### Exercise 2: Learning a Bayes Net (9 points)\n","\n","#### We will use the `pomegranate` library to train a Bayes Net to assess whether relaxing the assumption in Naive bayes (i.e., all features are independent given the target feature) could improve the classification model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html) for more details. "]},{"cell_type":"markdown","metadata":{"id":"Q6LhAf7J6t9c"},"source":["#### Exercise 2a: Create a categorical version of the dataset (1 point)\n","\n","#### Create categorical versions of the training and test sets by using equal-frequency binning with the number of bins set to 3 (as in Ex. 1c).\n","\n","#### <u>Use these datasets for training and evaluating the bayes net models in the following exercises.</u> \n","\n","**Note** This is done because pomegranate currently only supports bayes net over categorical features."]},{"cell_type":"code","metadata":{"id":"wwTP_x2I6t9c"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqPAKILl6t9c"},"source":["#### Exercise 2b: Construct a Bayes net (3 points)\n","\n","#### Construct and train a Bayes net in which the pregnancy (feature) node is a parent of the diabetes (feature) node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n","\n","#### Construct and train another Bayes net in which the glucose (feature) node is a parent of the diabetes (feature) node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n","\n","#### Report the training and test accuracies of each Bayes Net. Use get_performance (defined below) by passing in the trained bayes net, binned datasets, and specifying the index of the parent feature node."]},{"cell_type":"code","metadata":{"id":"ssRrH2un6t9d"},"source":["from pomegranate import *\n","\n","\"\"\"\n","X_train_binned: ndarray (# instances, # features) This is the binned version of the training set\n","y_train: 1darray (# instances,)\n","ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n","                            These indices correspond to features that are parent nodes of the diabetes node. \n","ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n","                            These indices correspond to features that are children nodes of the diabetes node.\n","                            \n","Returns a BayesianNetwork representing the trained bayes net\n","\"\"\"\n","def construct_and_train_bayes_net(X_train_binned,\n","                                  y_train,\n","                                  ind_chosen_parent_features=np.array([]), \n","                                  ind_chosen_child_features=np.array([]),\n","                                ):\n","    # parent nodes of diabetes\n","\n","    dist_by_parent_feature=[]\n","    state_by_parent_feature=[]\n","    if len(ind_chosen_parent_features)>0:\n","        parent_feature_names_chosen=FEATURE_NAMES[ind_chosen_parent_features]\n","\n","        for ft_index in ind_chosen_parent_features:\n","            ft_dist=DiscreteDistribution.from_samples(X_train_binned[:,ft_index])\n","            dist_by_parent_feature.append(ft_dist)\n","            state_by_parent_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n","        dist_by_parent_feature=np.array(dist_by_parent_feature)\n","        state_by_parent_feature=np.array(state_by_parent_feature)\n","\n","\n","    # diabetes node\n","    if len(ind_chosen_parent_features)>0:\n","        X_train_parent_features_binned_with_labels=np.concatenate((X_train_binned[:,ind_chosen_parent_features],\n","                                                                   np.expand_dims(y_train,axis=1)),axis=1)\n","        diabetes_dist=ConditionalProbabilityTable.from_samples(X_train_parent_features_binned_with_labels)\n","        # temporary workaround to properly initialize the distribution\n","        diabetes_dist=ConditionalProbabilityTable(diabetes_dist.parameters[0],dist_by_parent_feature.tolist())\n","    else:\n","        diabetes_dist=DiscreteDistribution.from_samples(y_train)\n","    diabetes_state=State(diabetes_dist, \"diabetes\")\n","\n","    # children node of diabetes\n","\n","    dist_by_child_feature=[]\n","    state_by_child_feature=[]    \n","    if len(ind_chosen_child_features)>0:\n","        child_feature_names_chosen=FEATURE_NAMES[ind_chosen_child_features]\n","\n","        for ft_index in ind_chosen_child_features:\n","            X_train_child_features_binned_with_labels=np.concatenate((np.expand_dims(y_train,axis=1),\n","                                                                        np.expand_dims(X_train_binned[:,ft_index],axis=1)),\n","                                                                     axis=1)\n","            ft_dist=ConditionalProbabilityTable.from_samples(X_train_child_features_binned_with_labels)\n","            ft_dist=ConditionalProbabilityTable(ft_dist.parameters[0],[diabetes_dist])\n","            dist_by_child_feature.append(ft_dist)\n","            state_by_child_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n","        dist_by_child_feature=np.array(dist_by_child_feature)\n","        state_by_child_feature=np.array(state_by_child_feature)\n","\n","\n","    pom_model = BayesianNetwork()\n","    pom_model.add_states(*list(state_by_parent_feature))\n","    pom_model.add_states(diabetes_state)\n","    pom_model.add_states(*list(state_by_child_feature))\n","\n","    for parent_index in np.arange(len(ind_chosen_parent_features)):\n","        pom_model.add_edge(state_by_parent_feature[parent_index],diabetes_state)\n","\n","    for child_index in np.arange(len(ind_chosen_child_features)):\n","        pom_model.add_edge(diabetes_state, state_by_child_feature[child_index])\n","\n","    pom_model.bake()\n","\n","    return pom_model\n","\n","\n","\"\"\"\n","pom_model: BayesianNetwork represents the trained bayes net model\n","X_train_binned: ndarray (# instances, # features) This is the binned training set\n","y_train: 1darray (# instances,)\n","X_test_binned: ndarray (# instances, # features) This is the binned test set\n","y_test: 1darray (# instances,)\n","ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n","                            These indices correspond to features that are parent nodes of the diabetes node. \n","ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n","                            These indices correspond to features that are children nodes of the diabetes node.\n","                            \n","Returns the training and test set accuracies attained by the bayes net model (pom_model)\n","\"\"\"\n","def get_performance(pom_model, X_train_binned, y_train, X_test_binned, y_test, \n","                    ind_chosen_parent_features=np.array([]), ind_chosen_child_features=np.array([])):\n","    nones_array=np.expand_dims(np.array([None]*len(X_train_binned)),axis=1)\n","    ind_diabetes_node=len(ind_chosen_parent_features)\n","    if len(ind_chosen_parent_features)>0:\n","        X_train_binned_with_none=X_train_binned[:,ind_chosen_parent_features]\n","        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,nones_array),axis=1)\n","    else:\n","        X_train_binned_with_none=nones_array\n","\n","    if len(ind_chosen_child_features)>0:\n","        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,\n","                                                X_train_binned[:,ind_chosen_child_features]),\n","                                               axis=1)\n","    pred_labels=np.array(pom_model.predict(X_train_binned_with_none),dtype='int64')[:,ind_diabetes_node]\n","    train_acc=accuracy_score(y_train, pred_labels)\n","\n","    nones_array=np.expand_dims(np.array([None]*len(X_test_binned)),axis=1)\n","    if len(ind_chosen_parent_features)>0:\n","        X_test_binned_with_none=X_test_binned[:,ind_chosen_parent_features]\n","        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,nones_array),axis=1)\n","    else:\n","        X_test_binned_with_none=nones_array\n","\n","    if len(ind_chosen_child_features)>0:\n","        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,\n","                                               X_test_binned[:,ind_chosen_child_features]),\n","                                               axis=1)\n","    pred_labels=np.array(pom_model.predict(X_test_binned_with_none),dtype='int64')[:,ind_diabetes_node]\n","    test_acc=accuracy_score(y_test, pred_labels)\n","    \n","    return train_acc, test_acc\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCtb-dPC6t9d"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Wa1O-r86t9e"},"source":["#### Comment on which feature seems more informative for predicting the presence of diabetes. (1 point)"]},{"cell_type":"markdown","metadata":{"id":"2Q1tGjCf6t9e"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"6AVRTJ8T6t9e"},"source":["#### Exercise 2c: Construct a Bayes net with parent and children nodes (3 points)\n","\n","#### Here, we'll implement a Bayes net with similar structure to one laid out in this [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6470852).\n","\n","#### Construct and train a Bayes net in which:\n","#### -the following features are all parents of the diabetes feature node (pregnancies, skin, bmi, pedigree, age).  \n","#### -the following features are all children of the diabetes feature node (glucose, bp, insulin)\n","#### Use construct_and_train_bayes_net by passing in the binned training dataset and specifying the indices of the parent feature nodes and indices of the children feature nodes.\n","\n","#### Report the training and test accuracy of the Bayes Net using get_performance by passing in the trained bayes net, binned datasets, and indices of the parent and children feature nodes."]},{"cell_type":"code","metadata":{"id":"Nc3eM-mI6t9e"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2_t0DHR6t9e"},"source":["#### Compare the performance of this Bayes net against the Bayes nets from Ex. 2b. (1 point)"]},{"cell_type":"markdown","metadata":{"id":"sNdYYylX6t9e"},"source":["TO DO"]},{"cell_type":"code","metadata":{"id":"YCGPTZJE6t9f"},"source":[""],"execution_count":null,"outputs":[]}]}