{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment4_ECE364_soln.ipynb","provenance":[],"collapsed_sections":["BASGXrOy4wat"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"E32UBMT7VKMm"},"source":["## Prepare python environment\n"]},{"cell_type":"code","metadata":{"id":"y_lm7Q-9VKMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryOZJYQa3PG0"},"source":["random_state=5 # use this to control randomness across runs e.g., dataset partitioning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BASGXrOy4wat"},"source":["## Preparing the Credit Card Fraud Detection dataset (2 points)\n","\n","The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n","See [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) for details of the dataset. We will post process the data to balance both the classes indicating whether the transaction is fraud or not."]},{"cell_type":"markdown","metadata":{"id":"URgO9HCN6RCl"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlKBDyEQDzrk","executionInfo":{"status":"ok","timestamp":1637095338642,"user_tz":300,"elapsed":1823,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"5dce620f-9805-4091-a231-377419f17e14"},"source":["# Download and load the dataset\n","import os\n","if not os.path.exists('creditcard.csv'): \n","    !wget https://raw.githubusercontent.com/JHA-Lab/ece364/main/dataset/creditcard.zip\n","    !unzip creditcard.zip\n","\n","df = pd.read_csv(\"creditcard.csv\")\n","print(df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n","0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n","1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n","2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n","3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n","4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n","\n","[5 rows x 31 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Npvx1EvED1Lo","executionInfo":{"status":"ok","timestamp":1637095384830,"user_tz":300,"elapsed":202,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"fe43b02f-c140-42a2-eede-3849ad7bcd6d"},"source":["# Check the datatype of each column\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 284807 entries, 0 to 284806\n","Data columns (total 31 columns):\n"," #   Column  Non-Null Count   Dtype  \n","---  ------  --------------   -----  \n"," 0   Time    284807 non-null  float64\n"," 1   V1      284807 non-null  float64\n"," 2   V2      284807 non-null  float64\n"," 3   V3      284807 non-null  float64\n"," 4   V4      284807 non-null  float64\n"," 5   V5      284807 non-null  float64\n"," 6   V6      284807 non-null  float64\n"," 7   V7      284807 non-null  float64\n"," 8   V8      284807 non-null  float64\n"," 9   V9      284807 non-null  float64\n"," 10  V10     284807 non-null  float64\n"," 11  V11     284807 non-null  float64\n"," 12  V12     284807 non-null  float64\n"," 13  V13     284807 non-null  float64\n"," 14  V14     284807 non-null  float64\n"," 15  V15     284807 non-null  float64\n"," 16  V16     284807 non-null  float64\n"," 17  V17     284807 non-null  float64\n"," 18  V18     284807 non-null  float64\n"," 19  V19     284807 non-null  float64\n"," 20  V20     284807 non-null  float64\n"," 21  V21     284807 non-null  float64\n"," 22  V22     284807 non-null  float64\n"," 23  V23     284807 non-null  float64\n"," 24  V24     284807 non-null  float64\n"," 25  V25     284807 non-null  float64\n"," 26  V26     284807 non-null  float64\n"," 27  V27     284807 non-null  float64\n"," 28  V28     284807 non-null  float64\n"," 29  Amount  284807 non-null  float64\n"," 30  Class   284807 non-null  int64  \n","dtypes: float64(30), int64(1)\n","memory usage: 67.4 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"l8BtplCfD43h"},"source":["#### There are a total of 284807 entries in this dataset with no missing values. First 30 columns are features and the last column indicates whether the transaction is fraud or not"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"npMqPm_kD7Va","executionInfo":{"status":"ok","timestamp":1637095389021,"user_tz":300,"elapsed":574,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"3bd63a8d-c9e5-4422-87e9-827d3f867aeb"},"source":["##### Look at some statistics of the data using the `describe` function\n","df.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>V10</th>\n","      <th>V11</th>\n","      <th>V12</th>\n","      <th>V13</th>\n","      <th>V14</th>\n","      <th>V15</th>\n","      <th>V16</th>\n","      <th>V17</th>\n","      <th>V18</th>\n","      <th>V19</th>\n","      <th>V20</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>284807.000000</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>2.848070e+05</td>\n","      <td>284807.000000</td>\n","      <td>284807.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>94813.859575</td>\n","      <td>3.919560e-15</td>\n","      <td>5.688174e-16</td>\n","      <td>-8.769071e-15</td>\n","      <td>2.782312e-15</td>\n","      <td>-1.552563e-15</td>\n","      <td>2.010663e-15</td>\n","      <td>-1.694249e-15</td>\n","      <td>-1.927028e-16</td>\n","      <td>-3.137024e-15</td>\n","      <td>1.768627e-15</td>\n","      <td>9.170318e-16</td>\n","      <td>-1.810658e-15</td>\n","      <td>1.693438e-15</td>\n","      <td>1.479045e-15</td>\n","      <td>3.482336e-15</td>\n","      <td>1.392007e-15</td>\n","      <td>-7.528491e-16</td>\n","      <td>4.328772e-16</td>\n","      <td>9.049732e-16</td>\n","      <td>5.085503e-16</td>\n","      <td>1.537294e-16</td>\n","      <td>7.959909e-16</td>\n","      <td>5.367590e-16</td>\n","      <td>4.458112e-15</td>\n","      <td>1.453003e-15</td>\n","      <td>1.699104e-15</td>\n","      <td>-3.660161e-16</td>\n","      <td>-1.206049e-16</td>\n","      <td>88.349619</td>\n","      <td>0.001727</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>47488.145955</td>\n","      <td>1.958696e+00</td>\n","      <td>1.651309e+00</td>\n","      <td>1.516255e+00</td>\n","      <td>1.415869e+00</td>\n","      <td>1.380247e+00</td>\n","      <td>1.332271e+00</td>\n","      <td>1.237094e+00</td>\n","      <td>1.194353e+00</td>\n","      <td>1.098632e+00</td>\n","      <td>1.088850e+00</td>\n","      <td>1.020713e+00</td>\n","      <td>9.992014e-01</td>\n","      <td>9.952742e-01</td>\n","      <td>9.585956e-01</td>\n","      <td>9.153160e-01</td>\n","      <td>8.762529e-01</td>\n","      <td>8.493371e-01</td>\n","      <td>8.381762e-01</td>\n","      <td>8.140405e-01</td>\n","      <td>7.709250e-01</td>\n","      <td>7.345240e-01</td>\n","      <td>7.257016e-01</td>\n","      <td>6.244603e-01</td>\n","      <td>6.056471e-01</td>\n","      <td>5.212781e-01</td>\n","      <td>4.822270e-01</td>\n","      <td>4.036325e-01</td>\n","      <td>3.300833e-01</td>\n","      <td>250.120109</td>\n","      <td>0.041527</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>-5.640751e+01</td>\n","      <td>-7.271573e+01</td>\n","      <td>-4.832559e+01</td>\n","      <td>-5.683171e+00</td>\n","      <td>-1.137433e+02</td>\n","      <td>-2.616051e+01</td>\n","      <td>-4.355724e+01</td>\n","      <td>-7.321672e+01</td>\n","      <td>-1.343407e+01</td>\n","      <td>-2.458826e+01</td>\n","      <td>-4.797473e+00</td>\n","      <td>-1.868371e+01</td>\n","      <td>-5.791881e+00</td>\n","      <td>-1.921433e+01</td>\n","      <td>-4.498945e+00</td>\n","      <td>-1.412985e+01</td>\n","      <td>-2.516280e+01</td>\n","      <td>-9.498746e+00</td>\n","      <td>-7.213527e+00</td>\n","      <td>-5.449772e+01</td>\n","      <td>-3.483038e+01</td>\n","      <td>-1.093314e+01</td>\n","      <td>-4.480774e+01</td>\n","      <td>-2.836627e+00</td>\n","      <td>-1.029540e+01</td>\n","      <td>-2.604551e+00</td>\n","      <td>-2.256568e+01</td>\n","      <td>-1.543008e+01</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>54201.500000</td>\n","      <td>-9.203734e-01</td>\n","      <td>-5.985499e-01</td>\n","      <td>-8.903648e-01</td>\n","      <td>-8.486401e-01</td>\n","      <td>-6.915971e-01</td>\n","      <td>-7.682956e-01</td>\n","      <td>-5.540759e-01</td>\n","      <td>-2.086297e-01</td>\n","      <td>-6.430976e-01</td>\n","      <td>-5.354257e-01</td>\n","      <td>-7.624942e-01</td>\n","      <td>-4.055715e-01</td>\n","      <td>-6.485393e-01</td>\n","      <td>-4.255740e-01</td>\n","      <td>-5.828843e-01</td>\n","      <td>-4.680368e-01</td>\n","      <td>-4.837483e-01</td>\n","      <td>-4.988498e-01</td>\n","      <td>-4.562989e-01</td>\n","      <td>-2.117214e-01</td>\n","      <td>-2.283949e-01</td>\n","      <td>-5.423504e-01</td>\n","      <td>-1.618463e-01</td>\n","      <td>-3.545861e-01</td>\n","      <td>-3.171451e-01</td>\n","      <td>-3.269839e-01</td>\n","      <td>-7.083953e-02</td>\n","      <td>-5.295979e-02</td>\n","      <td>5.600000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>84692.000000</td>\n","      <td>1.810880e-02</td>\n","      <td>6.548556e-02</td>\n","      <td>1.798463e-01</td>\n","      <td>-1.984653e-02</td>\n","      <td>-5.433583e-02</td>\n","      <td>-2.741871e-01</td>\n","      <td>4.010308e-02</td>\n","      <td>2.235804e-02</td>\n","      <td>-5.142873e-02</td>\n","      <td>-9.291738e-02</td>\n","      <td>-3.275735e-02</td>\n","      <td>1.400326e-01</td>\n","      <td>-1.356806e-02</td>\n","      <td>5.060132e-02</td>\n","      <td>4.807155e-02</td>\n","      <td>6.641332e-02</td>\n","      <td>-6.567575e-02</td>\n","      <td>-3.636312e-03</td>\n","      <td>3.734823e-03</td>\n","      <td>-6.248109e-02</td>\n","      <td>-2.945017e-02</td>\n","      <td>6.781943e-03</td>\n","      <td>-1.119293e-02</td>\n","      <td>4.097606e-02</td>\n","      <td>1.659350e-02</td>\n","      <td>-5.213911e-02</td>\n","      <td>1.342146e-03</td>\n","      <td>1.124383e-02</td>\n","      <td>22.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>139320.500000</td>\n","      <td>1.315642e+00</td>\n","      <td>8.037239e-01</td>\n","      <td>1.027196e+00</td>\n","      <td>7.433413e-01</td>\n","      <td>6.119264e-01</td>\n","      <td>3.985649e-01</td>\n","      <td>5.704361e-01</td>\n","      <td>3.273459e-01</td>\n","      <td>5.971390e-01</td>\n","      <td>4.539234e-01</td>\n","      <td>7.395934e-01</td>\n","      <td>6.182380e-01</td>\n","      <td>6.625050e-01</td>\n","      <td>4.931498e-01</td>\n","      <td>6.488208e-01</td>\n","      <td>5.232963e-01</td>\n","      <td>3.996750e-01</td>\n","      <td>5.008067e-01</td>\n","      <td>4.589494e-01</td>\n","      <td>1.330408e-01</td>\n","      <td>1.863772e-01</td>\n","      <td>5.285536e-01</td>\n","      <td>1.476421e-01</td>\n","      <td>4.395266e-01</td>\n","      <td>3.507156e-01</td>\n","      <td>2.409522e-01</td>\n","      <td>9.104512e-02</td>\n","      <td>7.827995e-02</td>\n","      <td>77.165000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>172792.000000</td>\n","      <td>2.454930e+00</td>\n","      <td>2.205773e+01</td>\n","      <td>9.382558e+00</td>\n","      <td>1.687534e+01</td>\n","      <td>3.480167e+01</td>\n","      <td>7.330163e+01</td>\n","      <td>1.205895e+02</td>\n","      <td>2.000721e+01</td>\n","      <td>1.559499e+01</td>\n","      <td>2.374514e+01</td>\n","      <td>1.201891e+01</td>\n","      <td>7.848392e+00</td>\n","      <td>7.126883e+00</td>\n","      <td>1.052677e+01</td>\n","      <td>8.877742e+00</td>\n","      <td>1.731511e+01</td>\n","      <td>9.253526e+00</td>\n","      <td>5.041069e+00</td>\n","      <td>5.591971e+00</td>\n","      <td>3.942090e+01</td>\n","      <td>2.720284e+01</td>\n","      <td>1.050309e+01</td>\n","      <td>2.252841e+01</td>\n","      <td>4.584549e+00</td>\n","      <td>7.519589e+00</td>\n","      <td>3.517346e+00</td>\n","      <td>3.161220e+01</td>\n","      <td>3.384781e+01</td>\n","      <td>25691.160000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Time            V1  ...         Amount          Class\n","count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n","mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n","std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n","min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n","25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n","50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n","75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n","max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n","\n","[8 rows x 31 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"D792olj-D_QL"},"source":["#### Visualize the distribution of fraudulent vs genuine transactions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"njzVsqZKEEZy","executionInfo":{"status":"ok","timestamp":1637095541277,"user_tz":300,"elapsed":137,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"c4397491-c80a-49b7-eaf8-8d5aef87b466"},"source":["# Make a pie chart showing transaction type\n","fig, ax = plt.subplots(1, 1)\n","ax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n","plt.axis('equal')\n","plt.ylabel('')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, '')"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"PDR0nrZiEGoW","executionInfo":{"status":"ok","timestamp":1637095548055,"user_tz":300,"elapsed":822,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"8e98c4ef-40a8-4de4-dda4-ac8ae52bcb85"},"source":["## Check fradulent activity over time (note: total time os 48 hours)\n","df[\"Time_Hr\"] = df[\"Time\"]/3600 # convert to hours\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\n","ax1.hist(df.Time_Hr[df.Class==0],bins=48,color='g',alpha=0.5)\n","ax1.set_title('Genuine')\n","ax2.hist(df.Time_Hr[df.Class==1],bins=48,color='r',alpha=0.5)\n","ax2.set_title('Fraud')\n","plt.xlabel('Time (hrs)')\n","plt.ylabel('# transactions')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, '# transactions')"]},"metadata":{},"execution_count":8},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x216 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"64QUegPwEKdX"},"source":["# Remove 'Time' feature as it is already captured when converting to hours\n","df = df.drop(['Time'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Ax7Nd7LEQZx"},"source":["#### Create a balanced dataset with 50% from each class"]},{"cell_type":"code","metadata":{"id":"tg-zhzqZEMZI"},"source":["fraud_indices = np.array(df[df.Class == 1].index) #indices corresponding to fraud transaction\n","genuine_ind = df[df.Class == 0].index #indices corresponding to genuine transaction\n","total_fraud_transactions = len(df[df.Class == 1]) # total transactions that were fraud\n","np.random.seed(0) # fix the random seed generator for consistent results\n","indices_genuine_transaction = np.random.choice(genuine_ind, total_fraud_transactions, replace = False)\n","indices_genuine_transaction = np.array(indices_genuine_transaction)\n","selected_balanced_indices = np.concatenate([fraud_indices,indices_genuine_transaction]) # indices for balanced data\n","balanced_data = df.iloc[selected_balanced_indices,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RydHugdIEWI4"},"source":["print(\"% genuine transactions: \",len(balanced_data[balanced_data.Class == 0])/len(balanced_data))\n","print(\"% fraud transactions: \",len(balanced_data[balanced_data.Class == 1])/len(balanced_data))\n","\n","# Make a pie chart showing transaction type\n","fig, ax = plt.subplots(1, 1)\n","ax.pie(balanced_data.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n","plt.axis('equal')\n","plt.ylabel('')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRhEcln77rIK"},"source":["### Extract target and descriptive features (0.5 points)"]},{"cell_type":"code","metadata":{"id":"blhp_Upk8E-Y"},"source":["# Store all the features from the data in X\n","X= balanced_data.drop('Class',axis=1)\n","# Store all the labels in y\n","y= balanced_data['Class']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdUFK3qG8Gnk"},"source":["# Convert data to numpy array\n","X = X.to_numpy()\n","y = y.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-JPYSnc8JQi"},"source":["### Create training and validation datasets (0.5 points)\n","\n","\n","We will split the dataset into training and validation set. Generally in machine learning, we split the data into training,\n","validation and test set (this will be covered in later chapters). The model with best performance on the validation set is used to evaluate perfromance on \n","the test set which is the unseen data. In this assignment, we will using `train set` for training and evaluate the performance on the `test set` for various \n","model configurations to determine the best hyperparameters (parameter setting yielding the best performance).\n","\n","Split the data into training and validation set using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation. This has been done for you."]},{"cell_type":"code","metadata":{"id":"BzTzI3iT8R5x"},"source":["X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=random_state) # 80% training and 20% validation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hc40XakM8ZjC"},"source":["### Preprocess the dataset (1 points)\n","\n","#### Preprocess by normalizing each feature to have zero mean and unit standard deviation. This can be done using `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n"]},{"cell_type":"code","metadata":{"id":"aBcVWz4M8qi3"},"source":["# Define the scaler for scaling the data\n","scaler = StandardScaler()\n","\n","# Normalize the training data\n","X_train = scaler.fit_transform(X_train)\n","\n","# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n","X_test = scaler.transform(X_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0ytKO6K6rW-"},"source":["## Training error-based models (18 points)\n","\n","\n","#### We will use the `sklearn` library to train a Multinomial Logistic Regression classifier and Support Vector Machines. \n"]},{"cell_type":"markdown","metadata":{"id":"hBAcnfZw6rW_"},"source":["### Exercise 1:  Learning a Multinomial Logistic Regression classifier (4 points)"]},{"cell_type":"markdown","metadata":{"id":"sZY5Qfz36rW_"},"source":["#### Use `sklearn`'s `SGDClassifier` to train a multinomial logistic regression classifier (i.e., using a one-versus-rest scheme) with Stochastic Gradient Descent. Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) for more details. \n","\n","#### Set the `random_state` as defined above,  increase the `n_iter_no_change` to 1000 and `max_iter` to 10000 to facilitate better convergence.  \n","\n","#### Report the model's accuracy over the training and test sets.\n"," "]},{"cell_type":"code","metadata":{"id":"JJ3SYc4J6rW_"},"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1hqKVGd6rW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636397416478,"user_tz":300,"elapsed":346,"user":{"displayName":"Bhishma Dedhia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3h2ED1Sw9S8uBgYLSPjN9sK1VektJqdLanJ29=s64","userId":"17386699114857927846"}},"outputId":"d1aeda59-76ed-4b6a-d3a8-1a24ccdb2905"},"source":["# Create Logistic Regression based classifier\n","clf = SGDClassifier(loss='log', random_state=random_state, n_iter_no_change=1000, max_iter=10000)\n","\n","# Train Classifer on training set\n","clf = clf.fit(X_train,y_train)\n","\n","#Predict the response for train dataset\n","y_pred_train = clf.predict(X_train)\n","\n","#Predict the response for test dataset\n","y_pred_test = clf.predict(X_test)\n","\n","print(\"train accuracy: %.2f\" % accuracy_score(y_train, y_pred_train))\n","print(\"test accuracy: %.2f\" % accuracy_score(y_test, y_pred_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train accuracy: 0.95\n","test accuracy: 0.93\n"]}]},{"cell_type":"markdown","metadata":{"id":"cV7PfP9G6rXA"},"source":["#### Explain any performance difference observed between the training and test datasets."]},{"cell_type":"markdown","metadata":{"id":"r080wHsO6rXA"},"source":["The classifier is slightly overfitting to the training dataset, resulting in lower accuracy on the test dataset."]},{"cell_type":"markdown","metadata":{"id":"IctolF7v6rXA"},"source":["### Exercise 2: Learning a Support Vector Machine (SVM) (14 points)"]},{"cell_type":"markdown","metadata":{"id":"7uWiMMUs6rXA"},"source":["#### Use `sklearn`'s `SVC` class to train an SVM (i.e., using a [one-versus-one scheme](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-one)). Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for more details. \n"," "]},{"cell_type":"code","metadata":{"id":"Jyp0Jj3x6rXA"},"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5dxvcSK6rXA"},"source":["#### Exercise 2a: Warm up (2 points)\n","\n","#### Train an SVM with a linear kernel. Set the  random_state to the value defined above. Keep all other parameters at their defaults.\n","\n","#### Report the model's accuracy over the training and test sets."]},{"cell_type":"code","metadata":{"id":"REdlLnyO6rXB","outputId":"8cece01e-254a-4e2a-a984-ac57eca236d4"},"source":["clf = SVC(kernel='linear', random_state=random_state)\n","clf.fit(X_train,y_train)\n","\n","print(\"training acc: %.2f\" % accuracy_score(y_train, clf.predict(X_train)))\n","print(\"test acc: %.2f\" % accuracy_score(y_test, clf.predict(X_test)))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["training acc: 0.95\n","test acc: 0.94\n"]}]},{"cell_type":"markdown","metadata":{"id":"dJELdAX36rXB"},"source":["#### Exercise 2b: Evaluate a polynomial kernel function (4 points)\n","\n","#### Try fitting an SVM with a polynomial kernel function and vary the degree among {1, 2, 3, 4}. Note that degree=1 yields a linear kernel. \n","\n","#### For each fitted classifier, report its accuracy over the training and test sets. \n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=100`.  When the data is not linearly separable, this encourages the model to fit the training data. Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"B6T71FR_6rXB","outputId":"7c55662d-0507-479c-bff6-d8eecc2d9e95"},"source":["\n","for degree in [1,2,3,4]:\n","    clf = SVC(kernel='poly', random_state=random_state, degree=degree, C=100)\n","    clf.fit(X_train,y_train)\n","    \n","    print(\"Poly kernel, degree: %d\" %degree)\n","    print(\"training acc: %.2f\" % accuracy_score(y_train, clf.predict(X_train)))\n","    print(\"test acc: %.2f\" % accuracy_score(y_test, clf.predict(X_test)))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Poly kernel, degree: 1\n","training acc: 0.95\n","test acc: 0.93\n","Poly kernel, degree: 2\n","training acc: 0.97\n","test acc: 0.86\n","Poly kernel, degree: 3\n","training acc: 0.99\n","test acc: 0.92\n","Poly kernel, degree: 4\n","training acc: 0.99\n","test acc: 0.89\n"]}]},{"cell_type":"markdown","metadata":{"id":"hdzaOyzg6rXB"},"source":["#### Explain the effect of increasing the degree of the polynomial."]},{"cell_type":"markdown","metadata":{"id":"vBeZEsy_6rXB"},"source":["Increasing the degree of the polynomial kernel function allows the model to better fit the training dataset. Consequently the model overfits to the training data and reduces generalization. "]},{"cell_type":"markdown","metadata":{"id":"GYfgKZuZ6rXB"},"source":["#### Exercise 2c: Evaluate the radial basis kernel function (6 points)\n","\n","#### Try fitting an SVM with a radial basis kernel function and vary the length-scale parameter given by $\\gamma$ among {0.1, 0.01,1,10, 100}. \n","\n","#### For each fitted classifier, report its accuracy over the training and test sets. \n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=100`.  When the data is not linearly separable, this encourages the model to fit the training data (read more [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)). Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"p3BCoYu66rXC","outputId":"60b427e9-ac3d-4a56-9aa8-c9ac91f8c369"},"source":["\n","for gamma in [0.01,0.1,1,10,100]:\n","    clf = SVC(kernel='rbf', random_state=random_state, gamma=gamma, C=100)\n","    clf.fit(X_train,y_train)\n","    \n","    print(\"RBF kernel, gamma: \", gamma)\n","    print(\"training acc: %.2f\" % accuracy_score(y_train, clf.predict(X_train)))\n","    print(\"test acc: %.2f\" % accuracy_score(y_test, clf.predict(X_test)))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["RBF kernel, gamma:  0.01\n","training acc: 0.97\n","test acc: 0.93\n","RBF kernel, gamma:  0.1\n","training acc: 1.00\n","test acc: 0.92\n","RBF kernel, gamma:  1\n","training acc: 1.00\n","test acc: 0.80\n","RBF kernel, gamma:  10\n","training acc: 1.00\n","test acc: 0.61\n","RBF kernel, gamma:  100\n","training acc: 1.00\n","test acc: 0.56\n"]}]},{"cell_type":"markdown","metadata":{"id":"dpzq9Ync6rXC"},"source":["#### Comment on the effect of increasing/reducing the length-scale parameter $\\gamma$. Also, compare the performance of the classifiers trained with RBF kernel function against those trained with the polynomial and linear kernel functions (i.e., Ex. 2b). "]},{"cell_type":"markdown","metadata":{"id":"M6_JKd8S6rXC"},"source":["Increasing $\\gamma$ degrades generalization. $\\gamma$ determines when points are deemed close and far. Larger $\\gamma$ reduces the radius of influence of select training examples (support vectors), so that data points are only affected by a few nearby support vectors during inference. Smaller $\\gamma$ increases the radius of influence of select training examples (support vectors), so that data points are also affected by more distant support vectors during inference. \n","\n","Increasing $\\gamma$ to values greater than 0.1 reduces generalization performance because the classifier effectively relies on a few training examples (support vectors) that are close to the data point in order to make its prediction. Learn more [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html).\n","\n","Classifiers trained with the RBF kernel function overfit the training data more than those trained with the polynomial and linear kernel functions. This is because the RBF kernel function maps into an infinite-dimensional feature space. The increased overfitting, esp. at large $\\gamma$, degrades generalization performance, resulting in less performant classifiers compared to classifiers trained with linear and polynomial kernel functions."]},{"cell_type":"markdown","metadata":{"id":"rhxJEJpS6rXC"},"source":["#### Exercise 2d: Briefly state the main difference between the logistic regression classifier and the SVM. (2 points)"]},{"cell_type":"markdown","metadata":{"id":"1jPMmTUQ6rXD"},"source":["SVMs are explicitly trained to learn decision boundaries with large margins separating the classes.  "]},{"cell_type":"code","metadata":{"id":"tPDWXglw6rXD"},"source":[""],"execution_count":null,"outputs":[]}]}