{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment4_ECE364.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"E32UBMT7VKMm"},"source":["## Prepare python environment\n"]},{"cell_type":"code","metadata":{"id":"y_lm7Q-9VKMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryOZJYQa3PG0"},"source":["random_state=5 # use this to control randomness across runs e.g., dataset partitioning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BASGXrOy4wat"},"source":["## Preparing the Credit Card Fraud Detection dataset (2 points)\n","\n","The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n","See [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) for details of the dataset. We will post process the data to balance both the classes indicating whether the transaction is fraud or not."]},{"cell_type":"markdown","metadata":{"id":"URgO9HCN6RCl"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"id":"tlKBDyEQDzrk"},"source":["# Download and load the dataset\n","import os\n","if not os.path.exists('creditcard.csv'): \n","    !wget https://raw.githubusercontent.com/JHA-Lab/ece364/main/dataset/creditcard.zip\n","    !unzip creditcard.zip\n","\n","df = pd.read_csv(\"creditcard.csv\")\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Npvx1EvED1Lo"},"source":["# Check the datatype of each column\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8BtplCfD43h"},"source":["#### There are a total of 284807 entries in this dataset with no missing values. First 30 columns are features and the last column indicates whether the transaction is fraud or not"]},{"cell_type":"code","metadata":{"id":"npMqPm_kD7Va"},"source":["##### Look at some statistics of the data using the `describe` function\n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D792olj-D_QL"},"source":["#### Visualize the distribution of fraudulent vs genuine transactions"]},{"cell_type":"code","metadata":{"id":"njzVsqZKEEZy"},"source":["# Make a pie chart showing transaction type\n","fig, ax = plt.subplots(1, 1)\n","ax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n","plt.axis('equal')\n","plt.ylabel('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDR0nrZiEGoW"},"source":["## Check fradulent activity over time (note: total time os 48 hours)\n","df[\"Time_Hr\"] = df[\"Time\"]/3600 # convert to hours\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\n","ax1.hist(df.Time_Hr[df.Class==0],bins=48,color='g',alpha=0.5)\n","ax1.set_title('Genuine')\n","ax2.hist(df.Time_Hr[df.Class==1],bins=48,color='r',alpha=0.5)\n","ax2.set_title('Fraud')\n","plt.xlabel('Time (hrs)')\n","plt.ylabel('# transactions')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"64QUegPwEKdX"},"source":["# Remove 'Time' feature as it is already captured when converting to hours\n","df = df.drop(['Time'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Ax7Nd7LEQZx"},"source":["#### Create a balanced dataset with 50% from each class"]},{"cell_type":"code","metadata":{"id":"tg-zhzqZEMZI"},"source":["fraud_indices = np.array(df[df.Class == 1].index) #indices corresponding to fraud transaction\n","genuine_ind = df[df.Class == 0].index #indices corresponding to genuine transaction\n","total_fraud_transactions = len(df[df.Class == 1]) # total transactions that were fraud\n","np.random.seed(0) # fix the random seed generator for consistent results\n","indices_genuine_transaction = np.random.choice(genuine_ind, total_fraud_transactions, replace = False)\n","indices_genuine_transaction = np.array(indices_genuine_transaction)\n","selected_balanced_indices = np.concatenate([fraud_indices,indices_genuine_transaction]) # indices for balanced data\n","balanced_data = df.iloc[selected_balanced_indices,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RydHugdIEWI4"},"source":["print(\"% genuine transactions: \",len(balanced_data[balanced_data.Class == 0])/len(balanced_data))\n","print(\"% fraud transactions: \",len(balanced_data[balanced_data.Class == 1])/len(balanced_data))\n","\n","# Make a pie chart showing transaction type\n","fig, ax = plt.subplots(1, 1)\n","ax.pie(balanced_data.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n","plt.axis('equal')\n","plt.ylabel('')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRhEcln77rIK"},"source":["### Extract target and descriptive features (0.5 points)"]},{"cell_type":"code","metadata":{"id":"blhp_Upk8E-Y"},"source":["# Store all the features from the data in X\n","X= # TODO \n","# Store all the labels in y\n","y= # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdUFK3qG8Gnk"},"source":["# Convert data to numpy array\n","X = # TODO \n","y = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-JPYSnc8JQi"},"source":["### Create training and validation datasets (0.5 points)\n","\n","\n","We will split the dataset into training and validation set. Generally in machine learning, we split the data into training,\n","validation and test set (this will be covered in later chapters). The model with best performance on the validation set is used to evaluate perfromance on \n","the test set which is the unseen data. In this assignment, we will using `train set` for training and evaluate the performance on the `test set` for various \n","model configurations to determine the best hyperparameters (parameter setting yielding the best performance).\n","\n","Split the data into training and validation set using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation. This has been done for you."]},{"cell_type":"code","metadata":{"id":"BzTzI3iT8R5x"},"source":["X_train,X_test,y_train,y_test = # TODO "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hc40XakM8ZjC"},"source":["### Preprocess the dataset (1 points)\n","\n","#### Preprocess by normalizing each feature to have zero mean and unit standard deviation. This can be done using `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n"]},{"cell_type":"code","metadata":{"id":"aBcVWz4M8qi3"},"source":["# Define the scaler for scaling the data\n","scaler = # TODO\n","\n","# Normalize the training data\n","X_train = # TODO\n","\n","# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n","X_test = # TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQ05ZCMf6qnc"},"source":["## Training error-based models (18 points)\n","\n","\n","#### We will use the `sklearn` library to train a Multinomial Logistic Regression classifier and Support Vector Machines. \n"]},{"cell_type":"markdown","metadata":{"id":"3FyfX3XN6qnd"},"source":["### Exercise 1:  Learning a Multinomial Logistic Regression classifier (4 points)"]},{"cell_type":"markdown","metadata":{"id":"EU4GBnfw6qnd"},"source":["#### Use `sklearn`'s `SGDClassifier` to train a multinomial logistic regression classifier (i.e., using a one-versus-rest scheme) with Stochastic Gradient Descent. Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) for more details. \n","\n","#### Set the `random_state` as defined above,  increase the `n_iter_no_change` to 1000 and `max_iter` to 10000 to facilitate better convergence.  \n","\n","#### Report the model's accuracy over the training and test sets.\n"," "]},{"cell_type":"code","metadata":{"id":"0YkkEBNt6qnd"},"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ge_9taaz6qnd"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XzAI_A8R6qne"},"source":["#### Explain any performance difference observed between the training and test datasets."]},{"cell_type":"markdown","metadata":{"id":"nvZpA9Yy6qne"},"source":["TODO"]},{"cell_type":"markdown","metadata":{"id":"sMyx1GzD6qne"},"source":["### Exercise 2: Learning a Support Vector Machine (SVM) (14 points)"]},{"cell_type":"markdown","metadata":{"id":"5BFa6aW16qnf"},"source":["#### Use `sklearn`'s `SVC` class to train an SVM (i.e., using a [one-versus-one scheme](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-one)). Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for more details. \n"," "]},{"cell_type":"code","metadata":{"id":"Dp1CVolq6qnf"},"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ag2GHzZ6qnf"},"source":["#### Exercise 2a: Warm up (2 points)\n","\n","#### Train an SVM with a linear kernel. Set the  random_state to the value defined above. Keep all other parameters at their defaults.\n","\n","#### Report the model's accuracy over the training and test sets."]},{"cell_type":"code","metadata":{"id":"fiUR_yq66qnf"},"source":["# TODO \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"us372Wzt6qng"},"source":["#### Exercise 2b: Evaluate a polynomial kernel function (4 points)\n","\n","#### Try fitting an SVM with a polynomial kernel function and vary the degree among {1, 2, 3, 4}. Note that degree=1 yields a linear kernel. \n","\n","#### For each fitted classifier, report its accuracy over the training and test sets. \n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=100`.  When the data is not linearly separable, this encourages the model to fit the training data. Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"tnP3znfE6qng"},"source":["# TODO "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFs6G1Zt6qng"},"source":["#### Explain the effect of increasing the degree of the polynomial."]},{"cell_type":"markdown","metadata":{"id":"RkmmRi436qng"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"iJQt1QBP6qnh"},"source":["#### Exercise 2c: Evaluate the radial basis kernel function (6 points)\n","\n","#### Try fitting an SVM with a radial basis kernel function and vary the length-scale parameter given by $\\gamma$ among {0.01, 0.1,1,10, 100}. \n","\n","#### For each fitted classifier, report its accuracy over the training and test sets. \n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=100`.  When the data is not linearly separable, this encourages the model to fit the training data (read more [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)). Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"dc0YDa4I6qnh"},"source":["# TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkBcadJV6qnh"},"source":["#### Comment on the effect of increasing/reducing the length-scale parameter $\\gamma$. Also, compare the performance of the classifiers trained with RBF kernel function against those trained with the polynomial and linear kernel functions (i.e., Ex. 2b). "]},{"cell_type":"markdown","metadata":{"id":"2DtQj8R66qni"},"source":["TO DO"]},{"cell_type":"markdown","metadata":{"id":"YHvWNLnB6qni"},"source":["#### Exercise 2d: Briefly state the main difference between the logistic regression classifier and the SVM. (2 points)"]},{"cell_type":"markdown","metadata":{"id":"Gxbqg7PL6qni"},"source":["TO DO"]},{"cell_type":"code","metadata":{"id":"4rqFPJIq6qnj"},"source":[""],"execution_count":null,"outputs":[]}]}